# import torch
# import torch.nn as nn
# import torch.optim as optim
# from src.models.vgg_cifar import vgg11_cifar100
# from src.data import get_data_loader

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# model = vgg11_cifar100(pretrained=True).to(device)

# criterion = nn.CrossEntropyLoss()
# optimizer = optim.SGD(
#     model.parameters(),
#     lr=0.01,
#     momentum=0.9,
#     weight_decay=5e-4
# )

# train_loader = get_data_loader(train=True, batch_size=128)
# test_loader  = get_data_loader(train=False, batch_size=128)

# for epoch in range(30):
#     model.train()
#     for x, y in train_loader:
#         x, y = x.to(device), y.to(device)
#         optimizer.zero_grad()
#         loss = criterion(model(x), y)
#         loss.backward()
#         optimizer.step()

#     print(f"[Epoch {epoch}] done")

# torch.save(model.state_dict(), "models/vgg11_cifar100_finetuned.pt")

# if __name__ == '__main__':
#     main()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
from src.models.vgg_cifar import vgg11_cifar100
from src.data import get_data_loader

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def main():
    # Ù…Ø¯Ù„
    model = vgg11_cifar100(pretrained=True).to(device)

    # loss Ùˆ optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(
        model.parameters(),
        lr=0.01,           # Ø´Ø±ÙˆØ¹ Ø¨Ø§ lr=0.01 Ø®ÙˆØ¨Ù‡ Ø¨Ø±Ø§ÛŒ fine-tune Ø§Ø² ImageNet
        momentum=0.9,
        weight_decay=5e-4
    )

    # ðŸ”¥ Scheduler Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡ Ø§ÛŒÙ†Ø¬Ø§
    scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)
    # Ù…Ø¹Ù†ÛŒ: Ù‡Ø± 50 epochØŒ lr Ø±Ùˆ Ø¶Ø±Ø¨Ø¯Ø± 0.1 Ú©Ù† (ÛŒØ¹Ù†ÛŒ 0.01 â†’ 0.001 â†’ 0.0001)

    # config Ø¨Ø±Ø§ÛŒ batch_size
    class Config:
        batch_size = 128
        resize = None  # ÛŒØ§ 32 Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ resize Ú©Ù†ÛŒ

    config = Config()

    train_loader = get_data_loader(batch_size=128, resize=None, train=True)
    test_loader = get_data_loader(batch_size=128, resize=None, train=False)

    num_epochs = 150  # ØªÙˆØµÛŒÙ‡: Ø­Ø¯Ø§Ù‚Ù„ 100-150 epoch Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø®ÙˆØ¨

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for i, (x, y) in enumerate(train_loader):
            x, y = x.to(device), y.to(device)

            optimizer.zero_grad()
            outputs = model(x)
            loss = criterion(outputs, y)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        # Ú†Ø§Ù¾ loss
        avg_loss = running_loss / len(train_loader)
        print(f"[Epoch {epoch+1:03d}/{num_epochs}] Loss: {avg_loss:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}")

        # ðŸ”¥ scheduler.step() Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø§ÛŒÙ†Ø¬Ø§ØŒ Ø¨Ø¹Ø¯ Ø§Ø² Ù‡Ø± epoch
        scheduler.step()

        # Ø§Ø®ØªÛŒØ§Ø±ÛŒ: Ù‡Ø± 20 epoch Ø¯Ù‚Øª Ø±ÙˆÛŒ test Ú†Ú© Ú©Ù†
        if (epoch + 1) % 20 == 0 or epoch == 0:
            model.eval()
            correct = 0
            total = 0
            with torch.no_grad():
                for x, y in test_loader:
                    x, y = x.to(device), y.to(device)
                    outputs = model(x)
                    _, predicted = torch.max(outputs, 1)
                    total += y.size(0)
                    correct += (predicted == y).sum().item()
            acc = 100 * correct / total
            print(f"    >>> Test Accuracy: {acc:.2f}%\n")

    # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
    save_path = "models/vgg11_cifar100_finetuned.pt"
    torch.save(model.state_dict(), save_path)
    print(f"\nTraining finished! Model saved to {save_path}")

if __name__ == '__main__':
    main()